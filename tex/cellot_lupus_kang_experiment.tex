\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}

\newcommand{\NoisyFlow}{NoisyFlow}
\newcommand{\CellOT}{CellOT}

\title{\NoisyFlow{} on a Real Single-Cell Dataset (Kang lupuspatients)}
\author{}
\date{}

\begin{document}
\maketitle

\section{Dataset and task}
We evaluate \NoisyFlow{} on the preprocessed \CellOT{} benchmark derived from the Kang lupuspatients scRNA-seq dataset (\texttt{datasets/scrna-lupuspatients/kang-hvg.h5ad}), which contains 28{,}871 cells and 1{,}000 highly-variable genes (HVGs). The dataset provides annotations for:
(i) \texttt{condition} $\in \{\texttt{ctrl},\texttt{stim}\}$,
(ii) \texttt{sample\_id} (patient id),
and (iii) \texttt{cell\_type} (8 immune cell types).

\paragraph{Domain shift.}
We treat \texttt{ctrl} as the \emph{source} domain and \texttt{stim} as the \emph{target} domain (control$\rightarrow$stimulated transport).
Features are reduced to 50 dimensions via PCA (\texttt{pca\_dim=50}) fit on the union of (i) all source cells from the selected clients and (ii) the target reference split.

\paragraph{Federated setting.}
Clients correspond to patients (\texttt{sample\_id}).
We hold out patient \texttt{101} as an out-of-distribution (OOD) test client and train on the first three remaining patients with at least 500 source cells each (\texttt{max\_clients=3}, \texttt{min\_cells\_per\_client=500}).

\paragraph{Evaluation task.}
The downstream task is supervised \textbf{cell-type classification on stimulated cells} from the held-out patient.
We restrict the number of labeled target-reference (\texttt{stim}) examples available for classifier training to $n_{\text{ref}}=50$ to reflect a limited-label target regime.

\section{Method (\NoisyFlow{})}
\NoisyFlow{} is a three-stage pipeline:
\begin{enumerate}
  \item \textbf{Stage 1 (client, conditional generator).}
  Each client trains a label-conditional flow-matching generator on its private source data (\texttt{ctrl}), using \texttt{cell\_type} as the class label.
  \item \textbf{Stage 2 (client, transport to target).}
  Each client learns an OT-style map into the public target reference distribution (\texttt{stim}).
  In this experiment we use the rectified-flow OT variant (\texttt{stage2.rectified\_flow.enabled=true}) with label-matched target sampling (\texttt{stage2.pair\_by\_label=true}).
  \item \textbf{Stage 3 (server, synthesis + classifier).}
  The server synthesizes labeled target-like samples by sampling from each client generator (Stage 1) and pushing forward with the learned transport map (Stage 2).
  A classifier is then trained on the synthetic labeled target-like data and evaluated on the OOD target test split.
\end{enumerate}

\section{Baselines}
We report three classifiers (all using the same evaluation split):
\begin{itemize}
  \item \textbf{Ref-only:} train on $n_{\text{ref}}$ labeled target-reference (\texttt{stim}) cells only.
  \item \textbf{Synth-only:} train on \NoisyFlow{} synthetic labeled target-like cells only.
  \item \textbf{Ref+Synth:} train on the union of the $n_{\text{ref}}$ labeled target-reference cells and the synthetic labeled target-like cells.
\end{itemize}

\section{Metrics}
\begin{itemize}
  \item \textbf{Accuracy:} cell-type classification accuracy on the held-out patient (\texttt{sample\_id=101}) stimulated test split.
  \item \textbf{Distributional alignment (MMD):} RBF MMD$^2$ between synthetic target-like samples and the held-out stimulated test cells, evaluated across a small set of kernel bandwidths and summarized by the minimum and mean.
\end{itemize}

\section{Results (single run, seed=0)}
Table~\ref{tab:kang-ref50-tuned} reports results for $n_{\text{ref}}=50$ labeled target-reference cells using the tuned configuration.

\begin{table}[t]
  \centering
  \begin{tabular}{lcccccc}
    \toprule
    $n_{\text{ref}}$ &
    Acc (Ref-only) &
    Acc (Synth-only) &
    Acc (Ref+Synth) &
    $\Delta$ &
    MMD$_{\min}$ &
    MMD$_{\text{mean}}$ \\
    \midrule
    50 & 0.852 & 0.903 & 0.905 & +0.053 & 0.00126 & 0.00727 \\
    \bottomrule
  \end{tabular}
  \caption{Kang lupuspatients OOD evaluation (\texttt{sample\_id=101}, stimulated cells). $\Delta=\text{Acc(Ref+Synth)}-\text{Acc(Ref-only)}$.}
  \label{tab:kang-ref50-tuned}
\end{table}

\section{Privacy--utility tradeoff (Stage~2 schemes)}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{privacy_curve_stage2_schemes_overlay.pdf}
  \caption{
    Privacy--utility curves on the Kang lupuspatients OOD setting (\texttt{sample\_id=101}) for three Stage~2 training schemes.
    Utility is \texttt{acc\_ref\_plus\_synth}: target test accuracy of a classifier trained on $n_{\text{ref}}=50$ labeled target-reference cells and transported synthetic data.
    Each point corresponds to a DP-SGD noise multiplier (with $\delta=10^{-5}$).
    For the synth-only scheme, Stage~2 is trained only on DP-synthetic source samples and public target-reference samples, so $\varepsilon$ reflects Stage~1 only (post-processing).
  }
  \label{fig:kang-privacy-utility-stage2-schemes}
\end{figure}

\paragraph{Description.}
We compare three Stage~2 training schemes and their privacy--utility tradeoffs.
The \emph{synth-only} scheme trains the OT map on DP synthetic source samples and public target-reference samples, so it does not consume additional privacy beyond Stage~1 (post-processing).
The \emph{private-only} scheme trains the OT map directly on private source samples using DP-SGD, which dominates the overall privacy cost at small privacy budgets and can lead to a noticeable utility drop in the strict-privacy regime.
Finally, the \emph{mixed} scheme trains the OT map with DP-SGD on a 1:1 mixture of private source and DP-synthetic source samples, which consistently improves utility at the same composed $\varepsilon$ (e.g., $\varepsilon\!\approx\!12.67$: 0.770 vs.\ 0.726 for private-only), indicating that DP synthetic augmentation can stabilize private OT training in low-label target settings.

\section{Reproducibility}
\paragraph{Data.}
Download and extract the \CellOT{} processed datasets ZIP:
\begin{verbatim}
python -m pip install anndata h5py
python scripts/fetch_cellot_datasets.py --dataset lupuspatients
\end{verbatim}

\paragraph{Configs.}
The experiments above correspond to:
\begin{verbatim}
python run.py --config configs/cellot_lupus_kang_rectifiedflow_ref50_tuned.yaml
\end{verbatim}

\end{document}
